{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnr0/InGen/blob/main/InGen_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing requirements"
      ],
      "metadata": {
        "id": "IgzAS-vWP5q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "St9ao0bH8B8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e14866b-9ffa-4794-d722-e10ff553495b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask==2.0.1\n",
            "  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting Jinja2>=3.0\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 32.0 MB/s \n",
            "\u001b[?25hCollecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting Werkzeug>=2.0\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from flask==2.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask==2.0.1) (2.0.1)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: MarkupSafe, Werkzeug, Jinja2, itsdangerous, flask\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 1.1.0\n",
            "    Uninstalling itsdangerous-1.1.0:\n",
            "      Successfully uninstalled itsdangerous-1.1.0\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 flask-2.0.1 itsdangerous-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_api==2.0\n",
            "  Downloading Flask_API-2.0-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.1 in /usr/local/lib/python3.7/dist-packages (from flask_api==2.0) (2.0.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1->flask_api==2.0) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1->flask_api==2.0) (3.1.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1->flask_api==2.0) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1->flask_api==2.0) (2.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->Flask>=1.1->flask_api==2.0) (2.1.1)\n",
            "Installing collected packages: flask-api\n",
            "Successfully installed flask-api-2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_socketio==5.1.0\n",
            "  Downloading Flask_SocketIO-5.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask_socketio==5.1.0) (2.0.1)\n",
            "Collecting python-socketio>=5.0.2\n",
            "  Downloading python_socketio-5.7.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_socketio==5.1.0) (3.1.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_socketio==5.1.0) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_socketio==5.1.0) (2.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_socketio==5.1.0) (2.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask_socketio==5.1.0) (2.1.1)\n",
            "Collecting bidict>=0.21.0\n",
            "  Downloading bidict-0.22.0-py3-none-any.whl (36 kB)\n",
            "Collecting python-engineio>=4.3.0\n",
            "  Downloading python_engineio-4.3.4-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-engineio, bidict, python-socketio, flask-socketio\n",
            "Successfully installed bidict-0.22.0 flask-socketio-5.1.0 python-engineio-4.3.4 python-socketio-5.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_cors==3.0.3\n",
            "  Downloading Flask_Cors-3.0.3-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask_cors==3.0.3) (1.15.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask_cors==3.0.3) (2.0.1)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_cors==3.0.3) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_cors==3.0.3) (2.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_cors==3.0.3) (2.2.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask_cors==3.0.3) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask_cors==3.0.3) (2.1.1)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-3.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffusers==0.4\n",
            "  Downloading diffusers-0.4.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: Pillow<10.0 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (7.1.2)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (4.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from diffusers==0.4) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.4) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.4) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.4) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.4) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.10.0->diffusers==0.4) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->diffusers==0.4) (3.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.4) (2022.9.24)\n",
            "Installing collected packages: huggingface-hub, diffusers\n",
            "Successfully installed diffusers-0.4.0 huggingface-hub-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, transformers, ftfy\n",
            "Successfully installed ftfy-6.1.1 tokenizers-0.13.1 transformers-4.23.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (7.7.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (3.0.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (7.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7) (5.3.4)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7) (6.1.12)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.2.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets<8,>=7) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8,>=7) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8,>=7) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (5.5.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (5.7.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (4.11.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (23.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (2.1.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (5.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (4.13.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (2.16.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (5.10.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7) (0.5.1)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gevent-websocket\n",
            "  Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
            "Collecting gevent\n",
            "  Downloading gevent-22.10.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket) (57.4.0)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet<2.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket) (1.1.3.post0)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: zope.interface, zope.event, gevent, gevent-websocket\n",
            "Successfully installed gevent-22.10.1 gevent-websocket-0.10.1 zope.event-4.5.0 zope.interface-5.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=f71f81cf86735100ad840baf9bed842363e8bf922b6c773a3360633f69516152\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask==2.0.1\n",
        "!pip install flask_api==2.0\n",
        "# !pip install python-socketio==4.6.0\n",
        "# !pip install python-engineio==3.13.2\n",
        "!pip install flask_socketio==5.1.0\n",
        "!pip install flask_cors==3.0.3\n",
        "# !pip install Werkzeug==0.16.1\n",
        "\n",
        "!pip install diffusers==0.4\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!pip install gevent-websocket\n",
        "\n",
        "\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken 1pDdjhcWhyh8VUFRTYCnGgCOI8g_3f56gBgxvREzT4gfAWXp2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging in with Huggingface Token"
      ],
      "metadata": {
        "id": "iiOa3AanP0V3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oUU23aKxhh-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271,
          "referenced_widgets": [
            "f73bd3c035254bf7b193a88fa7af319e",
            "ccb684fa0ef847e9a9302119c0d644db",
            "2bb1685d15164b91bbd1209102f47204",
            "3c902daf3b9348dc979915cbe92038e9",
            "3ecbc804d88447b4adb2b49dd73f6487",
            "8c5a1e76a78b44ceb95300025310ded1",
            "55053b19921f40f1bb82d62e8cba7bb8",
            "f3ae18f341ee44e282f9fab64f5547be",
            "37d1b21aaa95479397197e0e6cabff48",
            "6de440a0edf4468787343c6b2ef7a435",
            "8dc1fb07ec5d4334a67ea31a35971273",
            "b14bf86d1a784c1ca0b84284b10403f7",
            "127969de85664ffeb8c6d5d7a40cc9d1",
            "adec7004461b4a1f9d932f48bae93e3c"
          ]
        },
        "outputId": "d49f5b3c-4a05-426e-c4b0-6866fb5281f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Ngrok"
      ],
      "metadata": {
        "id": "5zmhlx0GP-AW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N0a2bMeTGY8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7310052-bf0d-4b20-a2c5-65d077fb64ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "# ngrok install\n",
        "!wget --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSrvWFBZhxE4"
      },
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TRGA9kn5hy0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7ac9b1-42b5-4dce-aafd-0bf0db90be43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'logit_scale', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", revision='fp16', torch_dtype=torch.float16, subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = text_encoder.half()\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", revision='fp16', torch_dtype=torch.float16, subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "from diffusers import DDIMScheduler\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "from torch import autocast\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device) \n",
        "\n",
        "embedded_text_prompts = {}\n",
        "\n",
        "\n",
        "\n",
        "def text_prompt_embed(t):\n",
        "  with torch.no_grad():\n",
        "    if t in embedded_text_prompts:\n",
        "      return embedded_text_prompts[t]\n",
        "    else: \n",
        "      text_input = tokenizer([t], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "      text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "      embedded_text_prompts[t] = text_embeddings\n",
        "      return text_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F2AGO2c7lGEc"
      },
      "outputs": [],
      "source": [
        "# variables that can change dynamically during generation\n",
        "directional_prompt_inputs = {}\n",
        "\n",
        "directional_prompt_inputs_proto = {}\n",
        "\n",
        "prompt_inputs = {}\n",
        "\n",
        "prompt_inputs_proto = {}\n",
        "\n",
        "prompts_whole_proto = {}\n",
        "\n",
        "guidance_scale = {}\n",
        "\n",
        "threads = {}\n",
        "\n",
        "sub_threads = {}\n",
        "\n",
        "gen_stop = {}\n",
        "\n",
        "latents_list = {}\n",
        "dir_prompt_list = {}\n",
        "prompt_list = {}\n",
        "guidance_scale_list = {}\n",
        "prompts_whole_list = {}\n",
        "\n",
        "lms = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Ngrok"
      ],
      "metadata": {
        "id": "6pNHKMSXQBE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hfcl69GCGf-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb105d07-a682-4f65-a5e2-da86f1d5c81d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://5b1c-104-196-1-175.ngrok.io\" -> \"http://localhost:5001\">"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "ngrok_tunnel = ngrok.connect(5001)\n",
        "ngrok_tunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run server"
      ],
      "metadata": {
        "id": "Lr7YmuadQK9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sz-dTsu58gxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea9920fd-c070-421a-e8bd-376b7d0a0ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 163, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 168, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "someone connected to websocket: noIYz0mrO2VKjgrDAAAB\n",
            "368 512\n",
            "(512, 704)\n",
            "1.7125086784362793\n",
            "1.7143683433532715\n",
            "1.7522854804992676\n",
            "tensor([950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300,\n",
            "        250, 200, 150, 100,  50,   0]) tensor([950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300,\n",
            "        250, 200, 150, 100,  50,   0])\n",
            "1.7544329166412354\n",
            "0 start 7\n",
            "unet pipeline0 1.7553260326385498 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 1.907485008239746\n",
            "unet pipeline2 1.9094815254211426\n",
            "post process 2\n",
            "post process 1.9109878540039062\n",
            "before emit 1.9550323486328125e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "1 start 7\n",
            "unet pipeline0 1.9134571552276611 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 2.5807595252990723\n",
            "unet pipeline2 2.5835320949554443\n",
            "post process 2\n",
            "post process 2.5851552486419678\n",
            "before emit 1.9073486328125e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "2 start 7\n",
            "unet pipeline0 2.5868983268737793 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.767960786819458\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-13:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e089bd0> instead of <flask.ctx.AppContext object at 0x7f592e0f4450>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 3.2181994915008545\n",
            "unet pipeline2 3.220879554748535\n",
            "post process 2\n",
            "post process 3.222484588623047\n",
            "before emit 1.811981201171875e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "3 start 7\n",
            "unet pipeline0 3.2283055782318115 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7296297550201416\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-14:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0f4450> instead of <flask.ctx.AppContext object at 0x7f592e0922d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 3.842824935913086\n",
            "unet pipeline2 3.84525465965271\n",
            "post process 2\n",
            "post process 3.8471570014953613\n",
            "before emit 1.6927719116210938e-05\n",
            "4 start 7\n",
            "torch.Size([1, 4, 88, 64])\n",
            "unet pipeline0 3.8487775325775146 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6995813846588135\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-15:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0922d0> instead of <flask.ctx.AppContext object at 0x7f586a3bd650>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 4.464411497116089\n",
            "unet pipeline2 4.467127084732056\n",
            "post process 2\n",
            "post process 4.468843460083008\n",
            "before emit 1.6689300537109375e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "5 start 7\n",
            "unet pipeline0 4.4748475551605225 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6998746395111084\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-16:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bd650> instead of <flask.ctx.AppContext object at 0x7f586a3bddd0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 5.10746693611145\n",
            "unet pipeline2 5.110284090042114\n",
            "post process 2\n",
            "post process 5.111924886703491\n",
            "before emit 1.8358230590820312e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "6 start 7\n",
            "unet pipeline0 5.118394374847412 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.716641902923584\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-17:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bddd0> instead of <flask.ctx.AppContext object at 0x7f592e168ed0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 5.729918003082275\n",
            "unet pipeline2 5.732658863067627\n",
            "post process 1\n",
            "post process 5.734221935272217\n",
            "before emit 1.811981201171875e-05\n",
            "7 start 7\n",
            "torch.Size([1, 4, 88, 64])\n",
            "unet pipeline0 5.738923072814941 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6976964473724365\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-18:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e168ed0> instead of <flask.ctx.AppContext object at 0x7f592e0898d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 6.370467662811279\n",
            "unet pipeline2 6.3733649253845215\n",
            "post process 1\n",
            "post process 6.375297784805298\n",
            "before emit 1.8835067749023438e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "8 start 7\n",
            "unet pipeline0 6.377021312713623 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7246191501617432\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-19:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0898d0> instead of <flask.ctx.AppContext object at 0x7f586a3802d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 7.0076963901519775\n",
            "unet pipeline2 7.010706424713135\n",
            "post process 1\n",
            "post process 7.0123610496521\n",
            "before emit 1.9073486328125e-05\n",
            "torch.Size([1, 4, 88, 64])9\n",
            " start 7\n",
            "unet pipeline0 7.018380165100098 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7153818607330322\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-20:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3802d0> instead of <flask.ctx.AppContext object at 0x7f586a3bded0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 7.640336275100708\n",
            "unet pipeline2 7.642748117446899\n",
            "post process 1\n",
            "post process 7.64426589012146\n",
            "before emit 3.62396240234375e-05\n",
            "10 start 7\n",
            "torch.Size([1, 4, 88, 64])unet pipeline0 7.649196147918701 True <class 'torch.Tensor'>\n",
            " torch.Size([]) True\n",
            "lets emit 0.7272675037384033\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-21:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bded0> instead of <flask.ctx.AppContext object at 0x7f586a380990>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 8.278562068939209\n",
            "unet pipeline2 8.281194686889648\n",
            "post process 1\n",
            "post process 8.283004760742188\n",
            "before emit 1.811981201171875e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "11 start 7\n",
            "unet pipeline0 8.288461446762085 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.71616530418396\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-22:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380990> instead of <flask.ctx.AppContext object at 0x7f586a380850>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 8.916007041931152\n",
            "unet pipeline2 8.918887376785278\n",
            "post process 1\n",
            "post process 8.920572519302368\n",
            "before emit 1.8358230590820312e-05\n",
            "12 start 7torch.Size([1, 4, 88, 64])\n",
            "\n",
            "unet pipeline0 8.926367282867432 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7401065826416016\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-23:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380850> instead of <flask.ctx.AppContext object at 0x7f592fc9dd10>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 9.544818878173828\n",
            "unet pipeline2 9.547939538955688\n",
            "post process 1\n",
            "post process 9.549235820770264\n",
            "before emit 3.600120544433594e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "13 start 7\n",
            "unet pipeline0 9.555726051330566 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7324912548065186\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-24:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592fc9dd10> instead of <flask.ctx.AppContext object at 0x7f586a2c55d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 10.19430923461914\n",
            "unet pipeline2 10.197273969650269\n",
            "post process 1\n",
            "post process 10.19935655593872\n",
            "before emit 1.7642974853515625e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "14 start 7\n",
            "unet pipeline0 10.202446460723877 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7488012313842773\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-25:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a2c55d0> instead of <flask.ctx.AppContext object at 0x7f586a2c59d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 10.837642669677734\n",
            "unet pipeline2 10.84149432182312\n",
            "post process 1\n",
            "post process 10.843705177307129\n",
            "before emit 2.288818359375e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "15 start 7\n",
            "unet pipeline0 10.850009679794312 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.7625596523284912\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-26:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a2c59d0> instead of <flask.ctx.AppContext object at 0x7f592e0a7ed0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 11.497828483581543\n",
            "unet pipeline2 11.49940276145935\n",
            "post process 1\n",
            "post process 11.501432657241821\n",
            "before emit16 start 7\n",
            "unet pipeline0 11.50331711769104 True <class 'torch.Tensor'> torch.Size([]) True\n",
            " 1.9788742065429688e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "lets emit 0.8014159202575684\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-27:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0a7ed0> instead of <flask.ctx.AppContext object at 0x7f586a380e90>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 12.131795167922974\n",
            "unet pipeline2 12.134586334228516\n",
            "post process 1\n",
            "post process 12.13710880279541\n",
            "before emit17 start 7\n",
            "unet pipeline0 12.138929605484009 True <class 'torch.Tensor'> torch.Size([]) True\n",
            " 2.1457672119140625e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "lets emit 0.7650949954986572\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-28:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380e90> instead of <flask.ctx.AppContext object at 0x7f586a3bdbd0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 12.771914720535278\n",
            "unet pipeline2 12.774252891540527\n",
            "post process 1\n",
            "post process 12.776803731918335\n",
            "before emit18 start 7\n",
            "unet pipeline0 12.77896523475647 True <class 'torch.Tensor'> torch.Size([]) True\n",
            " 2.2649765014648438e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "lets emit 0.768519401550293\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-29:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bdbd0> instead of <flask.ctx.AppContext object at 0x7f586a2c5b50>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 13.416479587554932\n",
            "unet pipeline2 13.418585300445557\n",
            "post process 1\n",
            "post process 13.420276165008545\n",
            "before emit 1.9788742065429688e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "19 start 7\n",
            "unet pipeline0 13.424229383468628 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.778461217880249\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-30:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a2c5b50> instead of <flask.ctx.AppContext object at 0x7f592e0f4810>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 14.066822290420532\n",
            "unet pipeline2 14.07013726234436\n",
            "post process 3\n",
            "post process 14.072184801101685\n",
            "before emit 2.2172927856445312e-05\n",
            "torch.Size([1, 4, 88, 64])\n",
            "lets emit 0.7514543533325195\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-31:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0f4810> instead of <flask.ctx.AppContext object at 0x7f592e0f47d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lets emit 0.5605683326721191\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-32:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e0f47d0> instead of <flask.ctx.AppContext object at 0x7f592e089bd0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 512\n",
            "(512, 512)\n",
            "0.2177276611328125\n",
            "0.21912503242492676\n",
            "0.23656344413757324\n",
            "tensor([950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300,\n",
            "        250, 200, 150, 100,  50,   0]) tensor([950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300,\n",
            "        250, 200, 150, 100,  50,   0])\n",
            "0.23854947090148926\n",
            "0 start 7\n",
            "unet pipeline0 0.23908472061157227 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 0.38521695137023926\n",
            "unet pipeline2 0.38735508918762207\n",
            "post process 2\n",
            "post process 0.38896799087524414\n",
            "before emit 2.288818359375e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "1 start 7\n",
            "unet pipeline0 0.39178943634033203 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 0.7965154647827148\n",
            "unet pipeline2 0.7994492053985596\n",
            "post process 2\n",
            "post process 0.801239013671875\n",
            "before emit 1.9311904907226562e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "2 start 7\n",
            "unet pipeline0 0.8077857494354248 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5145161151885986\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-34:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02a810> instead of <flask.ctx.AppContext object at 0x7f586a380050>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 1.228990077972412\n",
            "unet pipeline2 1.2321057319641113\n",
            "post process 2\n",
            "post process 1.233769178390503\n",
            "before emit3 start 7\n",
            " unet pipeline0 1.2367675304412842 1.9073486328125e-05\n",
            "torch.Size([1, 4, 64, 64])True <class 'torch.Tensor'> torch.Size([]) \n",
            "True\n",
            "lets emit 0.5235595703125\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-35:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380050> instead of <flask.ctx.AppContext object at 0x7f586a3bdb90>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 1.6765050888061523\n",
            "unet pipeline2 1.6789805889129639\n",
            "post process 2\n",
            "post process 1.6803219318389893\n",
            "before emit 1.9073486328125e-05\n",
            "4 starttorch.Size([1, 4, 64, 64])\n",
            " 7\n",
            "unet pipeline0 1.6830854415893555 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5704174041748047\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-36:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bdb90> instead of <flask.ctx.AppContext object at 0x7f586a2c59d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 2.095855712890625\n",
            "unet pipeline2 2.0984888076782227\n",
            "post process 2\n",
            "post process 2.100128412246704\n",
            "before emit 5 start 7\n",
            "1.7881393432617188e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "unet pipeline0 2.10636830329895 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5242893695831299\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-37:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a2c59d0> instead of <flask.ctx.AppContext object at 0x7f592e02a750>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 2.532532215118408\n",
            "unet pipeline2 2.536263942718506\n",
            "post process 2\n",
            "post process 2.538193702697754\n",
            "before emit 2.0265579223632812e-05\n",
            "6torch.Size([1, 4, 64, 64])\n",
            " start 7\n",
            "unet pipeline0 2.5447745323181152 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5468263626098633\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-38:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02a750> instead of <flask.ctx.AppContext object at 0x7f586a380650>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 2.969712018966675\n",
            "unet pipeline2 2.97255539894104\n",
            "post process 1\n",
            "post process 2.974071979522705\n",
            "before emit 1.9788742065429688e-05\n",
            "torch.Size([1, 4, 64, 64])7 start 7\n",
            "\n",
            "unet pipeline0 2.9792301654815674 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5718100070953369\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-39:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380650> instead of <flask.ctx.AppContext object at 0x7f592e02a390>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 3.4014461040496826\n",
            "unet pipeline2 3.4042775630950928\n",
            "post process 1\n",
            "post process 3.406062364578247\n",
            "before emit 1.7404556274414062e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "8 start 7\n",
            "unet pipeline0 3.411432981491089 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5521833896636963\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-40:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02a390> instead of <flask.ctx.AppContext object at 0x7f592e02a650>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 3.827653646469116\n",
            "unet pipeline2 3.830679178237915\n",
            "post process 1\n",
            "post process 3.8324203491210938\n",
            "before emit 1.7881393432617188e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "9 start 7\n",
            "unet pipeline0 3.83874773979187 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5517082214355469\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-41:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02a650> instead of <flask.ctx.AppContext object at 0x7f592e02ab10>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 4.260998487472534\n",
            "unet pipeline2 4.2637083530426025\n",
            "post process 1\n",
            "post process 4.265481233596802\n",
            "before emit 1.71661376953125e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "10 start 7\n",
            "unet pipeline0 4.269876956939697 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5354979038238525\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-42:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02ab10> instead of <flask.ctx.AppContext object at 0x7f586a24d450>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 4.702425956726074\n",
            "unet pipeline2 4.705378532409668\n",
            "post process 1\n",
            "post process 4.707047939300537\n",
            "before emit 1.6927719116210938e-05\n",
            "11 start 7\n",
            "torch.Size([1, 4, 64, 64])\n",
            "unet pipeline0 4.713529348373413 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5449719429016113\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-43:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a24d450> instead of <flask.ctx.AppContext object at 0x7f586a380550>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 5.134238958358765\n",
            "unet pipeline2 5.137201309204102\n",
            "post process 1\n",
            "post process 5.138771295547485\n",
            "before emit 1.9311904907226562e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "12 start 7\n",
            "unet pipeline0 5.145213842391968 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5388510227203369\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-44:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a380550> instead of <flask.ctx.AppContext object at 0x7f586a28e210>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 5.569207191467285\n",
            "unet pipeline2 5.57189416885376\n",
            "post process 1\n",
            "post process 5.573503255844116\n",
            "before emit 4.696846008300781e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "13 start 7\n",
            "unet pipeline0 5.576526165008545 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.543773889541626\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-45:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a28e210> instead of <flask.ctx.AppContext object at 0x7f586a24d810>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 5.999767303466797\n",
            "unet pipeline2 6.002731561660767\n",
            "post process 1\n",
            "post process 6.004390716552734\n",
            "before emit 1.9073486328125e-05\n",
            "torch.Size([1, 4, 64, 64])14 start \n",
            "7\n",
            "unet pipeline0 6.011321067810059 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5690691471099854\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-46:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a24d810> instead of <flask.ctx.AppContext object at 0x7f586a24dc50>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 6.447195529937744\n",
            "unet pipeline2 6.450148582458496\n",
            "post process 1\n",
            "post process 6.4516870975494385\n",
            "before emit 1.8358230590820312e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "15 start 7\n",
            "unet pipeline0 6.455592155456543 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.5719361305236816\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-47:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a24dc50> instead of <flask.ctx.AppContext object at 0x7f586a28e8d0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 6.8895180225372314\n",
            "unet pipeline2 6.892668008804321\n",
            "post process 1\n",
            "post process 6.894184350967407\n",
            "before emit 1.6450881958007812e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "16 start 7\n",
            "unet pipeline0 6.900686264038086 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6053378582000732\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-48:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a28e8d0> instead of <flask.ctx.AppContext object at 0x7f586a28ed90>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 7.3294126987457275\n",
            "unet pipeline2 7.331539154052734\n",
            "post process 1\n",
            "post process 7.333198308944702\n",
            "before emit 1.8835067749023438e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "17 start 7\n",
            "unet pipeline0 7.338308095932007 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6256012916564941\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-49:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a28ed90> instead of <flask.ctx.AppContext object at 0x7f586a28e590>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 7.773026466369629\n",
            "unet pipeline2 7.77599310874939\n",
            "post process 1\n",
            "post process 7.777583837509155\n",
            "before emit 1.811981201171875e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "18 start 7\n",
            "unet pipeline0 7.783629655838013 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6157436370849609\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-50:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a28e590> instead of <flask.ctx.AppContext object at 0x7f592e02a090>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 8.211523294448853\n",
            "unet pipeline2 8.214226007461548\n",
            "post process 1\n",
            "post process 8.215895891189575\n",
            "before emit 1.811981201171875e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "19 start 7\n",
            "unet pipeline0 8.222553730010986 True <class 'torch.Tensor'> torch.Size([]) True\n",
            "lets emit 0.6122395992279053\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-51:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f592e02a090> instead of <flask.ctx.AppContext object at 0x7f586a24dad0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.HalfTensor\n",
            "unet pipeline1 8.672089338302612\n",
            "unet pipeline2 8.674787521362305\n",
            "post process 3\n",
            "post process 8.676310300827026\n",
            "before emit 1.9073486328125e-05\n",
            "torch.Size([1, 4, 64, 64])\n",
            "lets emit 0.6221089363098145\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-52:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a24dad0> instead of <flask.ctx.AppContext object at 0x7f586a3bdd50>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lets emit 0.5084507465362549\n",
            "thread end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-53:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 172, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 474, in __exit__\n",
            "    self.auto_pop(exc_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 460, in auto_pop\n",
            "    self.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 447, in pop\n",
            "    app_ctx.pop(exc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/ctx.py\", line 253, in pop\n",
            "    assert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\n",
            "AssertionError: Popped wrong app context.  (<flask.ctx.AppContext object at 0x7f586a3bdd50> instead of <flask.ctx.AppContext object at 0x7f592e02a810>)\n",
            "\n",
            "KeyboardInterrupt\n",
            "2022-10-27T21:56:38Z\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8226f318237a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;31m# socketio.run(app, host='0.0.0.0', debug=False, port=5001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0mhttp_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWSGIServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWebSocketHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m \u001b[0mhttp_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/baseserver.py\u001b[0m in \u001b[0;36mserve_forever\u001b[0;34m(self, stop_timeout)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mGreenlet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_cevent.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_cevent.Event.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_abstract_linkable.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_abstract_linkable.AbstractLinkable._wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_abstract_linkable.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_abstract_linkable.AbstractLinkable._wait_core\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_abstract_linkable.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_abstract_linkable.AbstractLinkable._wait_core\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_abstract_linkable.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_abstract_linkable.AbstractLinkable._AbstractLinkable__wait_to_be_notified\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_abstract_linkable.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_abstract_linkable.AbstractLinkable._switch_to_hub\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_greenlet_primitives.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gevent/_gevent_c_greenlet_primitives.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mgevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msrc/gevent/_gevent_c_greenlet_primitives.pxd\u001b[0m in \u001b[0;36mgevent._gevent_c_greenlet_primitives._greenlet_switch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from flask_socketio import SocketIO, emit\n",
        "from flask import Flask, request, copy_current_request_context\n",
        "from flask_cors import CORS\n",
        "from random import gauss, random\n",
        "# from threading import Thread, Event\n",
        "from time import sleep\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from threading import Thread\n",
        "from gevent.pywsgi import WSGIServer\n",
        "from geventwebsocket.handler import WebSocketHandler\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = 'secret!'\n",
        "\n",
        "socketio = SocketIO(app, cors_allowed_origins='*', async_mode='gevent', ping_interval=1, max_http_buffer_size=10e8)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "\n",
        "@app.route('/hello')\n",
        "def hello():\n",
        "    return \"Hello World!\"\n",
        "\n",
        "\n",
        "# Handle the webapp connecting to the websocket\n",
        "@socketio.on('connect')\n",
        "def test_connect():\n",
        "    print('someone connected to websocket:', request.sid)\n",
        "    emit('connect', {'data': 'Connected! ayy'})\n",
        "    \n",
        "@socketio.on('disconnect')\n",
        "def test_connect():\n",
        "    print(request.sid)\n",
        "    print('someone disconnected to websocket')\n",
        "    emit('disconnect', {'data': 'Disconnected! ayy'})\n",
        "\n",
        "# Handle the webapp connecting to the websocket, including namespace for testing\n",
        "@socketio.on('connect', namespace='/devices')\n",
        "def test_connect2():\n",
        "    print('someone connected to websocket!')\n",
        "    emit('responseMessage', {'data': 'Connected devices! ayy'})\n",
        "\n",
        "\n",
        "def preprocess_mask(mask):\n",
        "    mask = mask.convert(\"L\")\n",
        "    w, h = mask.size\n",
        "    if w < h:\n",
        "      h = int(h*(512/w))\n",
        "      w = 512\n",
        "    else:\n",
        "      w = int(w*(512/h))\n",
        "      h = 512\n",
        "      \n",
        "\n",
        "    # if w > 512:\n",
        "    #   h = int(h * (512/w))\n",
        "    #   w = 512\n",
        "    # if h > 512:\n",
        "    #   w = int(w*(512/h))\n",
        "    #   h = 512\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 64, (w, h)) \n",
        "    w //= 8\n",
        "    h //= 8\n",
        "\n",
        "    mask = mask.resize((w, h), resample=Image.LANCZOS)\n",
        "\n",
        "    # mask = np.array(mask).astype(np.float32) / 255.0\n",
        "    mask = np.array(mask).astype(np.float16) / 255.0\n",
        "    mask = np.tile(mask, (4,1,1))\n",
        "    mask = mask[None].transpose(0,1,2,3)\n",
        "    mask[np.where(mask !=0.0)]=1.0\n",
        "    mask = torch.from_numpy(mask)\n",
        "    return mask\n",
        "\n",
        "def preprocess(image):\n",
        "    image = image.convert('RGB')\n",
        "    w, h = image.size\n",
        "    print(w,h)\n",
        "    if w < h:\n",
        "      h = int(h*(512/w))\n",
        "      w = 512\n",
        "    else:\n",
        "      w = int(w*(512/h))\n",
        "      h = 512\n",
        "    # if w > 512:\n",
        "    #   h = int(h * (512/w))\n",
        "    #   w = 512\n",
        "    # if h > 512:\n",
        "    #   w = int(w*(512/h))\n",
        "    #   h = 512\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64, 32 can sometimes result in tensor mismatch errors\n",
        "\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "    print(image.size)\n",
        "    # image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "def numpy_to_pil(images):\n",
        "    \"\"\"\n",
        "    Convert a numpy image or a batch of images to a PIL image.\n",
        "    \"\"\"\n",
        "    if images.ndim == 3:\n",
        "            images = images[None, ...]\n",
        "    images = (images * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image).convert('RGBA') for image in images]\n",
        "    return pil_images\n",
        "\n",
        "@socketio.on('test')\n",
        "def handle_message_t(message):\n",
        "    print(message)\n",
        "\n",
        "@socketio.on('gen_stop')\n",
        "def gen_stop_message_t(message):\n",
        "    gen_stop[message['stroke_id']] = True\n",
        "\n",
        "@socketio.on('guidance_scale_update')\n",
        "def guidance_scale_message_t(message):\n",
        "  guidance_scale[message['stroke_id']] = message['guidance_scale']\n",
        "\n",
        "@socketio.on('prompts_update')\n",
        "def prompts_message_t(message):\n",
        "  text_prompts = message['text_prompts']\n",
        "  text_prompt_weights = message['text_prompt_weights']\n",
        "\n",
        "  text_prompt_embedding = None\n",
        "  tpw = 0\n",
        "  for tp_idx, text_prompt in enumerate(text_prompts):\n",
        "    # print('inloop1', time.time()-now)\n",
        "    cur_embedding = text_prompt_embed(text_prompt)\n",
        "    # print('inloop2', time.time()-now)\n",
        "    if text_prompt_embedding==None:\n",
        "      text_prompt_embedding = cur_embedding*text_prompt_weights[tp_idx]\n",
        "    else:\n",
        "      text_prompt_embedding = text_prompt_embedding + cur_embedding*text_prompt_weights[tp_idx]\n",
        "    tpw = tpw + text_prompt_weights[tp_idx]\n",
        "    # print('inloop3', time.time()-now)\n",
        "  text_prompt_embedding = text_prompt_embedding/tpw\n",
        "\n",
        "  # uncond_embeddings = text_prompt_embed('')\n",
        "  # text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding])\n",
        "\n",
        "  prompts_whole_proto[message['stroke_id']] = message['prompts_proto']\n",
        "  prompt_inputs[message['stroke_id']] = text_prompt_embedding #text_embeddings\n",
        "  prompt_inputs_proto[message['stroke_id']] = message['selected_prompts_proto']\n",
        "\n",
        "@socketio.on('directional_prompts_update')\n",
        "def directional_prompts_message_t(message):\n",
        "  directional_prompts = message['directional_prompts']\n",
        "\n",
        "  directional_vector = None\n",
        "  for directional_prompt in directional_prompts:\n",
        "    if directional_prompt['value']==0:\n",
        "      continue\n",
        "    if directional_vector == None:\n",
        "      directional_vector = float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "    else:\n",
        "      directional_vector = directional_vector + float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "  directional_prompt_inputs[message['stroke_id']] = directional_vector\n",
        "  directional_prompt_inputs_proto[message['stroke_id']] = message['directional_prompts_proto']        \n",
        "\n",
        "\n",
        "@socketio.on('gen_start')\n",
        "@torch.no_grad()\n",
        "def handle_message(message):\n",
        "    @copy_current_request_context\n",
        "    def handle_message_thread(message):\n",
        "      try:\n",
        "        with torch.no_grad():\n",
        "          now = time.time()\n",
        "\n",
        "          gen_stop[message['stroke_id']] = False\n",
        "\n",
        "          guidance_scale[message['stroke_id']] = message['guidance_scale']\n",
        "          text_prompts = message['text_prompts']\n",
        "          text_prompt_weights = message['text_prompt_weights']\n",
        "          directional_prompts = message['directional_prompts']\n",
        "\n",
        "          if message['gen_tick']==0:\n",
        "            latents_list[message['stroke_id']] = []\n",
        "            dir_prompt_list[message['stroke_id']] = []\n",
        "            prompt_list[message['stroke_id']] = []\n",
        "            guidance_scale_list[message['stroke_id']] = []\n",
        "            prompts_whole_list[message['stroke_id']] = []\n",
        "          else:\n",
        "            # pop things until the tick\n",
        "            latents_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            dir_prompt_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            prompt_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            guidance_scale_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "            prompts_whole_list[message['stroke_id']] = [None]*message['gen_tick']\n",
        "\n",
        "          layer_img_o = Image.open(BytesIO(base64.b64decode(message['layer_img'].split(\",\",1)[1])))\n",
        "          area_img = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "          area_img_o = Image.open(BytesIO(base64.b64decode(message['area_img'].split(\",\",1)[1])))\n",
        "          area_img.paste(area_img_o, (0,0), area_img_o)\n",
        "          \n",
        "          \n",
        "\n",
        "\n",
        "          overcoat_ratio = message['overcoat_ratio']\n",
        "          seed = message['seed']\n",
        "          generator = torch.Generator(device='cuda', )\n",
        "          generator.manual_seed(seed) \n",
        "\n",
        "          # set mask from area_img\n",
        "          area_mask = preprocess_mask(area_img)\n",
        "          area_mask = area_mask.to(torch_device)\n",
        "\n",
        "          \n",
        "\n",
        "          # add black or white background to the layer image\n",
        "          layer_img_back = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "          layer_img_back.paste(layer_img_o, (0, 0), layer_img_o)\n",
        "          layer_img_back.convert('RGB')\n",
        "          layer_img = preprocess(layer_img_back)\n",
        "          init_latents = vae.encode(layer_img.to(torch_device)).latent_dist.sample()\n",
        "          init_latents = 0.18215 * init_latents\n",
        "\n",
        "          init_latents = init_latents.half()\n",
        "\n",
        "          noise = torch.randn(init_latents.shape, generator=generator, device=torch_device, dtype=torch.float16)\n",
        "\n",
        "          num_inference_steps = message['steps']\n",
        "          scheduler.set_timesteps(num_inference_steps)\n",
        "          t_init = scheduler.timesteps[message['gen_tick']]\n",
        "          # if message['gen_tick'] < int((1-overcoat_ratio)*num_inference_steps):\n",
        "          layer_array = np.copy(np.asarray(layer_img_o))\n",
        "\n",
        "          alphas = np.ones(layer_array[:,:,3,None].shape)*255\n",
        "          layer_array = np.concatenate((layer_array[:,:,3,None], layer_array[:,:,3,None], layer_array[:,:,3,None], alphas), axis = 2)\n",
        "          layer_array = np.array(layer_array, dtype = np.uint8)\n",
        "      \n",
        "          layer_img_mask = Image.fromarray(layer_array)\n",
        "          layer_mask = preprocess_mask(layer_img_mask)\n",
        "          layer_mask = layer_mask.to(torch_device)\n",
        "\n",
        "          last_mask = None\n",
        "          if 'last_img' in message:\n",
        "            last_img = Image.new(\"RGBA\", layer_img_o.size, \"WHITE\")\n",
        "            last_img_o = Image.open(BytesIO(base64.b64decode(message['last_img'].split(\",\",1)[1])))\n",
        "            last_img.paste(last_img_o, (0,0), last_img_o)\n",
        "            last_mask = preprocess_mask(last_img)\n",
        "            last_mask = last_mask.to(torch_device)\n",
        "            last_mask = 1+last_mask-area_mask\n",
        "            # last_mask = layer_mask\n",
        "\n",
        "            last_latents = torch.Tensor(message['last_latents'])\n",
        "            last_latents = last_latents.to(torch_device)\n",
        "            # lms.append(last_latents)\n",
        "            # lms.append(last_img)\n",
        "\n",
        "\n",
        "          print(time.time()-now)\n",
        "          if message['gen_tick']==0:   \n",
        "            \n",
        "            # latents = noise # start from the purse noise\n",
        "\n",
        "            latents = (1-area_mask)* (1-layer_mask) * noise + (1-(1-area_mask)* (1-layer_mask)) * init_latents # In / Out\n",
        "            latents = (0.1) * latents + 0.9 * noise\n",
        "            \n",
        "            # scheduler.add_noise(latents, noise, t-1)\n",
        "          else:\n",
        "            # print('stored latent is used')\n",
        "            latents = torch.Tensor(message['latents'])\n",
        "            latents = latents.to(torch_device)\n",
        "\n",
        "\n",
        "          # print(latents.size())\n",
        "\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          # set directional prompt embeddings\n",
        "          directional_vector = None\n",
        "          for directional_prompt in directional_prompts:\n",
        "            if directional_prompt['value']==0:\n",
        "              continue\n",
        "            if directional_vector == None:\n",
        "              directional_vector = float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "            else:\n",
        "              directional_vector = directional_vector + float(directional_prompt['value'])/100.0 * (text_prompt_embed(directional_prompt['promptB'])-text_prompt_embed(directional_prompt['promptA']))\n",
        "          \n",
        "          text_prompt_embedding = None\n",
        "          tpw = 0\n",
        "          for tp_idx, text_prompt in enumerate(text_prompts):\n",
        "            # print('inloop1', time.time()-now)\n",
        "            cur_embedding = text_prompt_embed(text_prompt)\n",
        "            # print('inloop2', time.time()-now)\n",
        "            if text_prompt_embedding==None:\n",
        "              text_prompt_embedding = cur_embedding*text_prompt_weights[tp_idx]\n",
        "            else:\n",
        "              text_prompt_embedding = text_prompt_embedding + cur_embedding*text_prompt_weights[tp_idx]\n",
        "            tpw = tpw + text_prompt_weights[tp_idx]\n",
        "            # print('inloop3', time.time()-now)\n",
        "          text_prompt_embedding = text_prompt_embedding/tpw\n",
        "\n",
        "          uncond_embeddings = text_prompt_embed('')\n",
        "          # if directional_vector!=None:\n",
        "          #   text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding+directional_vector])\n",
        "          # else:\n",
        "          #   text_embeddings = torch.cat([uncond_embeddings, text_prompt_embedding])\n",
        "\n",
        "          directional_prompt_inputs[message['stroke_id']] = directional_vector\n",
        "          directional_prompt_inputs_proto[message['stroke_id']] = message['directional_prompts_proto']\n",
        "          prompts_whole_proto[message['stroke_id']] = message['prompts_proto']\n",
        "          prompt_inputs[message['stroke_id']] = text_prompt_embedding\n",
        "          prompt_inputs_proto[message['stroke_id']] = message['selected_prompts_proto']\n",
        "          # print(uncond_embeddings.size(), text_prompt_embedding.size())\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          # num inference steps should be fixed\n",
        "          gen_duration = np.min([message['gen_tick']+message['gen_duration'], message['steps']])\n",
        "          gen_duration = np.max([1, gen_duration])\n",
        "          \n",
        "          print(scheduler.timesteps, scheduler.timesteps[message['gen_tick']:])\n",
        "          # sub_threads[message['stroke_id']] = Thread(target=sendoutIntermediate, args = (message['gen_tick'], layer_img_o, area_img_o, message['area_img'], message['stroke_id'], gen_duration))\n",
        "          # sub_threads[message['stroke_id']].start()\n",
        "\n",
        "          print(time.time()-now)\n",
        "\n",
        "          \n",
        "          for i, t in enumerate(scheduler.timesteps[message['gen_tick']:gen_duration]):\n",
        "            print(i, 'start', guidance_scale[message['stroke_id']])\n",
        "            if gen_stop[message['stroke_id']]:\n",
        "              return\n",
        "            # Do something about generation\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            # predict the noise residual\n",
        "            with torch.no_grad():\n",
        "              if directional_prompt_inputs[message['stroke_id']] == None:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, prompt_inputs[message['stroke_id']]])\n",
        "              else:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, prompt_inputs[message['stroke_id']]+directional_prompt_inputs[message['stroke_id']]])\n",
        "              print('unet pipeline0', time.time()-now, latent_model_input.is_cuda, type(t), t.shape, text_embeddings.is_cuda)\n",
        "              noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "              print(noise_pred.type())\n",
        "            # perform guidance\n",
        "            print('unet pipeline1', time.time()-now)\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            \n",
        "            noise_pred = noise_pred_uncond + guidance_scale[message['stroke_id']] * (noise_pred_text - noise_pred_uncond)\n",
        "            latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "            print('unet pipeline2', time.time()-now)\n",
        "\n",
        "            # t_noise = torch.randn(latents.shape, device=torch_device)\n",
        "            if t > 1:\n",
        "              # when over overcoat ratio\n",
        "              if last_mask == None:\n",
        "                if i >= int((1-overcoat_ratio)*num_inference_steps):\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                  proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                  sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                  sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                  sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                  sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                  init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                  latents = init_latents_proper * area_mask    +    latents * (1-area_mask)\n",
        "                  print('post process 1')\n",
        "                else:\n",
        "                  proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                  sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                  sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                  sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                  sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                  init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                  latents = (1-area_mask)* (1-layer_mask) * latents + (1-(1-area_mask)* (1-layer_mask)) * init_latents_proper # In / Out\n",
        "                  print('post process 2')\n",
        "              else:\n",
        "                print('this is last')\n",
        "                # if i >= int((1-overcoat_ratio)*num_inference_steps):\n",
        "                  # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                proto_prod = torch.ones(1,1,1,1, device=torch_device, dtype=torch.float16)\n",
        "                sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                last_latents_proper = scheduler.add_noise(last_latents, noise, t-1)\n",
        "                # latents = init_latents_proper * (last_mask+area_mask-1)    +    latents * (1-area_mask) + last_latents_proper * (1-last_mask)\n",
        "                latents = init_latents_proper * (area_mask-layer_mask*area_mask)    +    latents * (1-area_mask) + last_latents_proper * (layer_mask* area_mask)\n",
        "                # lms.append(init_latents_proper * (last_mask+area_mask-1))\n",
        "                # lms.append(latents * (1-area_mask) )\n",
        "                # lms.append(last_latents_proper * (1-last_mask))\n",
        "                print('post process 1')\n",
        "                # else:\n",
        "                #   proto_prod = torch.ones(1,1,1,1, device=torch_device)\n",
        "                #   sqrt_alpha_prod = scheduler.alphas_cumprod[t-1] ** 0.5\n",
        "                #   sqrt_one_minus_alpha_prod = (1 - scheduler.alphas_cumprod[t-1]) ** 0.5\n",
        "                #   sqrt_alpha_prod=sqrt_alpha_prod * proto_prod\n",
        "                #   sqrt_one_minus_alpha_prod=sqrt_one_minus_alpha_prod * proto_prod\n",
        "                #   init_latents_proper = sqrt_alpha_prod * init_latents + sqrt_one_minus_alpha_prod * noise\n",
        "                #   last_latents_proper = scheduler.add_noise(last_latents, noise, t-1)\n",
        "                #   # init_latents_proper = scheduler.add_noise(init_latents, noise, t-1)\n",
        "                #   # latents = (1-area_mask)* (1-layer_mask) * latents + (last_mask-(1-area_mask)* (1-layer_mask)) * init_latents_proper + last_latents_proper * (1-last_mask) # In / Out\n",
        "                #   latents = init_latents_proper * (layer_mask-(1-last_mask))    +    latents * (1-last_mask) *(1-area_mask) + last_latents_proper * (1-last_mask)\n",
        "                #   print('post process 2')\n",
        "                \n",
        "              # when below overcoat ratio\n",
        "            else:\n",
        "              if last_mask==None:\n",
        "                latents = init_latents * area_mask    +    latents * (1-area_mask)\n",
        "              else:\n",
        "                # latents = init_latents* (last_mask+area_mask-1)    +    latents * (1-area_mask) + last_latents * (1-last_mask)\n",
        "                latents = init_latents * (area_mask-layer_mask*area_mask)    +    latents * (1-area_mask) + last_latents * (layer_mask*area_mask)\n",
        "              print('post process 3')\n",
        "            print('post process', time.time()-now)\n",
        "\n",
        "            latents_list[message['stroke_id']].append(latents)\n",
        "            dir_prompt_list[message['stroke_id']].append(directional_prompt_inputs_proto[message['stroke_id']])\n",
        "            prompt_list[message['stroke_id']].append(prompt_inputs_proto[message['stroke_id']])\n",
        "            guidance_scale_list[message['stroke_id']].append(guidance_scale[message['stroke_id']])\n",
        "            prompts_whole_list[message['stroke_id']].append(prompts_whole_proto[message['stroke_id']])\n",
        "\n",
        "            int_thread = Thread(target=sendoutIntermediate, args = (message['gen_tick']+i, layer_img_o, area_img_o, message['area_img'], message['stroke_id'], gen_duration))\n",
        "            int_thread.start()\n",
        "      except:\n",
        "        emit('gen_failed', {'data':'gen failed!'})\n",
        "\n",
        "\n",
        "\n",
        "    @copy_current_request_context\n",
        "    def sendoutIntermediate(gen_tick, layer_img_o, area_img_o, area_img_d, stroke_id, gen_duration):\n",
        "      idx = gen_tick\n",
        "      ta = time.time()\n",
        "      with torch.no_grad():\n",
        "          \n",
        "          \n",
        "        if gen_stop[stroke_id]:\n",
        "          return\n",
        "        print('before emit', time.time()-ta)\n",
        "        latents = latents_list[stroke_id][idx]\n",
        "        dir_prompts_ = dir_prompt_list[stroke_id][idx]\n",
        "        prompts_ = prompt_list[stroke_id][idx]\n",
        "        prompts_whole_ = prompts_whole_list[stroke_id][idx]\n",
        "        guidance_scale_ = guidance_scale_list[stroke_id][idx]\n",
        "        print(latents.shape)\n",
        "        \n",
        "        output_img = 1 / 0.18215 * latents\n",
        "        output_img = vae.decode(output_img).sample\n",
        "        output_img = (output_img / 2 + 0.5).clamp(0, 1)\n",
        "        output_img = output_img.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        output_img = numpy_to_pil(output_img)[0]\n",
        "        output_img = output_img.resize((layer_img_o.size[0], layer_img_o.size[1]), resample=Image.LANCZOS)\n",
        "        output_array = np.asarray(output_img)\n",
        "        output_array = np.copy(output_array)\n",
        "        latents_rt = latents.tolist()\n",
        "        # print('within3', time.time()-now)\n",
        "\n",
        "        area_array = np.asarray(area_img_o)\n",
        "        area_array = np.where(area_array==255, 255, 0)\n",
        "        # print(area_array.shape, gaussian.shape)\n",
        "        output_array[:,:,3] = area_array[:,:,3]\n",
        "        # print('within4', time.time()-now)\n",
        "        output_img = Image.fromarray(output_array)\n",
        "        # print('within5', time.time()-now)\n",
        "        \n",
        "        # print('here?')\n",
        "        buffered = BytesIO()\n",
        "        output_img.save(buffered, format=\"PNG\")\n",
        "        output_img_send = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "        print('lets emit', time.time()-ta)\n",
        "        # emit('test', {'data':'test'+str(idx)})\n",
        "        idx = idx+1\n",
        "        emit('intermediate_gen', {'data':area_img_d.split(\",\",1)[0]+','+output_img_send, 'gen_tick':idx, 'stroke_id': stroke_id, 'latents': latents_rt, 'selected_prompts': prompts_, 'prompts':prompts_whole_, 'directional_prompts': dir_prompts_, 'guidance_scale': guidance_scale_})\n",
        "        print('thread end!')\n",
        "\n",
        "    threads[message['stroke_id']] = Thread(target=handle_message_thread , args = [message])\n",
        "    threads[message['stroke_id']].start()\n",
        "    emit('test', {'data':'gen started!'})\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "# socketio.run(app, host='0.0.0.0', debug=False, port=5001)\n",
        "http_server = WSGIServer(('',5001), app, handler_class=WebSocketHandler)\n",
        "http_server.serve_forever()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f73bd3c035254bf7b193a88fa7af319e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccb684fa0ef847e9a9302119c0d644db",
              "IPY_MODEL_2bb1685d15164b91bbd1209102f47204",
              "IPY_MODEL_3c902daf3b9348dc979915cbe92038e9",
              "IPY_MODEL_3ecbc804d88447b4adb2b49dd73f6487"
            ],
            "layout": "IPY_MODEL_8c5a1e76a78b44ceb95300025310ded1"
          }
        },
        "ccb684fa0ef847e9a9302119c0d644db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55053b19921f40f1bb82d62e8cba7bb8",
            "placeholder": "​",
            "style": "IPY_MODEL_f3ae18f341ee44e282f9fab64f5547be",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2bb1685d15164b91bbd1209102f47204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_37d1b21aaa95479397197e0e6cabff48",
            "placeholder": "​",
            "style": "IPY_MODEL_6de440a0edf4468787343c6b2ef7a435",
            "value": ""
          }
        },
        "3c902daf3b9348dc979915cbe92038e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8dc1fb07ec5d4334a67ea31a35971273",
            "style": "IPY_MODEL_b14bf86d1a784c1ca0b84284b10403f7",
            "tooltip": ""
          }
        },
        "3ecbc804d88447b4adb2b49dd73f6487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127969de85664ffeb8c6d5d7a40cc9d1",
            "placeholder": "​",
            "style": "IPY_MODEL_adec7004461b4a1f9d932f48bae93e3c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "8c5a1e76a78b44ceb95300025310ded1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "55053b19921f40f1bb82d62e8cba7bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ae18f341ee44e282f9fab64f5547be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d1b21aaa95479397197e0e6cabff48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de440a0edf4468787343c6b2ef7a435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc1fb07ec5d4334a67ea31a35971273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14bf86d1a784c1ca0b84284b10403f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "127969de85664ffeb8c6d5d7a40cc9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adec7004461b4a1f9d932f48bae93e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}